# -*- coding: utf-8 -*-
"""handwritten_text_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19xWqG-GXHrCGOWdQU_XbncL-kvHp7IIz

```
###MNIST text detection using CNN and PyTorch
i.e handwiritten 28*28 grascale image from 0 to 9
```
"""

# Commented out IPython magic to ensure Python compatibility.
#importing files
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torchvision.utils import make_grid

import numpy as np
import pandas as pd
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
# %matplotlib inline

import time

#converting MNIST image files into tensor of 4-Dimensions(no of images, hight, width, color channel )
transform = transforms.ToTensor()

#Train data
train_data = datasets.MNIST(root='/cnn_data', train = True, download = True, transform= transform)

#Test data
test_data = datasets.MNIST(root='/cnn_data', train = False, download = True, transform= transform)

train_data

test_data

#Create a small batch size  for images
train_loader = DataLoader(train_data, batch_size = 10, shuffle=True)
test_loader = DataLoader(train_data, batch_size = 10, shuffle=False)

#Model Class
class ConvolutionalNetwork(nn.Module):
  def __init__(self):
    super().__init__()
    #convulational layer
    self.conv1 = nn.Conv2d(1, 6, 3, 1)
    self.conv2 = nn.Conv2d(6, 16, 3, 1)
    #fully connected layer
    self.fc1 = nn.Linear(5*5*16, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.max_pool2d(x,2,2) #2x2 kernal and stride 2
    #second pass
    x = F.relu(self.conv2(x))
    x = F.max_pool2d(x,2,2)

    #review to flatten it out
    x =x.view(-1, 16*5*5) #negative one so that we can vary the batch size

    #fully connected layer
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return F.log_softmax(x, dim=1)

# creating the instance of our model
torch.manual_seed(68)
model = ConvolutionalNetwork()
model

#Loss functiion optmizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

start_time = time.time()

#create variable to track things
epochs = 5
train_losses = []
test_losses = []
train_correct = []
test_correct = []

#for loop of epochs
for i in range(epochs):
  trn_corr = 0
  tst_corr = 0

  #train
  for b,(X_train, y_train) in enumerate(train_loader):
    b+=1
    y_pred = model(X_train)
    loss = criterion(y_pred, y_train)

    predicted = torch.max(y_pred.data, 1)[1] #add up the number of correct prediction. Indexed off the first point
    batch_corr = (predicted == y_train).sum() # how many we got correct from this batch
    trn_corr += batch_corr

    #update out parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    #print out some results
    if b%600 == 0:
      print(f'Epoch: {i} Batch: {b} Loss: {loss.item()}')

  train_losses.append(loss)
  train_correct.append(trn_corr)

  # test
  with torch.no_grad(): #no gradient update so we dont update our weight with test
    for b,(X_test, y_test) in enumerate(test_loader):
      y_val = model(X_test)
      predicted = torch.max(y_val.data, 1)[1] # adding up correct predictions
      tst_corr +=  (predicted == y_test).sum() #T=1 F=0 and sum away

  loss = criterion(y_val, y_test)
  test_losses.append(loss)
  test_correct.append(tst_corr)



current_time = time.time()
total_time = current_time - start_time
print(f'Training Took: {total_time/60} minutes!')

# graph the loss at epoch
train_losses = [t1.item() for t1 in train_losses]
plt.plot(train_losses, label = 'Training loss')
plt.plot(test_losses, label = 'Validation Loss')
plt.title('Loss at Epoch')
plt.legend()

#graph of accuracy at the end of each epch
plt.plot([t/600 for t in train_correct], label = 'training accuracy')
plt.plot([t/100 for t in test_correct], label = 'Validation aaccuracy')
plt.title('accuracy at the end of each epoch')
plt.legend()